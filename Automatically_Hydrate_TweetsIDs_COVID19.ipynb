{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automatically_Hydrate_TweetsIDs_COVID19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalolan/COVID19_Tweets_Dataset/blob/master/Automatically_Hydrate_TweetsIDs_COVID19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQiik2HFUtqL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<center><b>© 2020. Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu/  Malolan Vasu, vasum@lafayette.edu <b><center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwMczOoPVH6e",
        "colab_type": "text"
      },
      "source": [
        "**UPDATED ON 4/17/2020**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmFBUngZaxt7",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/lopezbec/COVID19_Tweets_Dataset/blob/master/Automatically_Hydrate_TweetsIDs_COVID19.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrI81Xn6wh0d",
        "colab_type": "text"
      },
      "source": [
        "# **Notebook to automatically \"hydrate\" tweets-ID**\n",
        "\n",
        "This notebook will allow you to automatically hydrate the tweets-ID from our [COVID19_Tweets_dataset GitHub repository](https://github.com/lopezbec/COVID19_Tweets_Dataset).\n",
        "\n",
        "\n",
        "You can run this notebook directly on the cloud using Google Colab [(see how to tutorials)]( https://colab.research.google.com/notebooks/welcome.ipynb#scrollTo=xitplqMNk_Hc) and Google Drive.\n",
        "\n",
        "In order to hydrate the tweet-IDs using [TWARC](https://github.com/DocNow/twarc) you need to create a [Twitter Developer Account]( https://developer.twitter.com/en/apply-for-access).\n",
        "\n",
        "First run the Initialization block to set up the required libraries. Then, run the Configuration block and choose your required configuration for tweet IDs to hydrate (dates needed, keywords needed). Finally, run the Hydrate block.  In the case that the code unexpectedly stops in the hydrate phase, you only need to run Initialization and Hydration. You aren't required to run configuration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbVmAxxGHUjO",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Zr5kB6wknZ",
        "colab_type": "text"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHzFzMoOV0tu",
        "colab_type": "text"
      },
      "source": [
        "The code will clone the repository and place it in your Google drive. Here you need to type where in your Google Drive you would like the information stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeimRLrjwpRr",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "d493b5fd-5c44-4f23-d8a9-d9ac0ee4e799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@title Set up Directory { run: \"auto\"}\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive \n",
        "from IPython.display import clear_output\n",
        "drive.mount('/content/gdrive')\n",
        "working_directory = 'My Drive/Research/form' #@param {type:\"string\"}\n",
        "wd=\"/content/gdrive/\"+working_directory\n",
        "os.chdir(wd)\n",
        "\n",
        "dirpath = os.getcwd()\n",
        "print(\"current directory is : \" + dirpath)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "current directory is : /content/gdrive/My Drive/Research/form\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDzd7FUKFviv",
        "colab_type": "text"
      },
      "source": [
        "### Install twarc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvHm-ggFubK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pip install twarc\n",
        "%pip install jsonlines\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gBPP8oJGDqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check if TWARC was installed correctly on the Virtual Machine\n",
        "%pip show twarc\n",
        "%pip show jsonlines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEsz47hdFHtW",
        "colab_type": "text"
      },
      "source": [
        "### Twitter API Keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et0_5DEEFNpW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Insert API Keys here { run : \"auto\"}\n",
        "from twarc import Twarc\n",
        "\n",
        "# These keys are received after applying for a twitter developer account\n",
        "\n",
        "consumer_key = \"\" #@param {type:\"string\"}\n",
        "consumer_secret = \"\" #@param {type:\"string\"}\n",
        "access_token = \"\" #@param {type:\"string\"}\n",
        "access_token_secret = \"\" #@param {type:\"string\"}\n",
        "\n",
        "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyS66qo29K_g",
        "colab_type": "text"
      },
      "source": [
        "### Clone Github Repository onto Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AwIDi53Wq9k",
        "colab_type": "text"
      },
      "source": [
        "This will clone the repository and place it in your Google drive. If the repository has already been cloned, it will update the repo and pull changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYcBEQ9Nv65y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if cd COVID19_Tweets_Dataset; then git pull; else git clone https://github.com/lopezbec/COVID19_Tweets_Dataset.git COVID19_Tweets_Dataset; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASdYyTqI9RUG",
        "colab_type": "text"
      },
      "source": [
        "# Configuration: Choose Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fj9ydtn9aIL",
        "colab_type": "text"
      },
      "source": [
        "### Keywords\n",
        "\n",
        "We recommend you run the code for a few keywords and create many output.csv files. Then, you can copy them to a different directory, and merge them when  required. Selecting too many keywords and dates might result in data too large to be stored in memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGdsBbsHxrcG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Check Keywords to Hydrate { run: \"auto\" }\n",
        "coronavirus = True #@param {type:\"boolean\"}\n",
        "virus = False #@param {type:\"boolean\"}\n",
        "covid = False #@param {type:\"boolean\"}\n",
        "ncov19 = False #@param {type:\"boolean\"}\n",
        "ncov2019 = False #@param {type:\"boolean\"}\n",
        "keyword_dict = {\"coronavirus\": coronavirus, \"virus\": virus, \"covid\": covid, \"ncov19\": ncov19, \"ncov2019\": ncov2019}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSNFUOZN952G",
        "colab_type": "text"
      },
      "source": [
        "### Get Number of Tweets by Dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u1DI1_oX6TV",
        "colab_type": "text"
      },
      "source": [
        "If you are running this in Google Colab we recommend starting with a range of just 1 day, do to the Google Colab 12hr limit and Twitter API limit. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-UwuBDZ-xW1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter range of dates to Hydrate { run: \"auto\" }\n",
        "start_date = '2020-01-22' #@param {type:\"date\"}\n",
        "end_date = '2020-01-23' #@param {type:\"date\"}\n",
        "\n",
        "\n",
        "import datetime as dt\n",
        "files = []\n",
        "covid_loc = \"COVID19_Tweets_Dataset\"\n",
        "# Looks at each volder\n",
        "for folder in os.listdir(covid_loc):\n",
        "    foldername = os.fsdecode(folder)\n",
        "    # The folder name is a keyword. We continue for keywords selected above\n",
        "    if keyword_dict.get(foldername.split()[0].lower()) == True:\n",
        "        folderpath = os.path.join(covid_loc, foldername)\n",
        "        # Each file is of the format [keyword]_yyyy_mm_dd.txt\n",
        "        for file in os.listdir(folderpath):\n",
        "            filename = os.fsdecode(file)\n",
        "            date = filename[filename.index(\"_\")+1:filename.index(\".\")]\n",
        "\n",
        "            # If the date is within the required range, it is added to the\n",
        "            # list of files to read.\n",
        "            if (dt.datetime.strptime(start_date, \"%Y-%m-%d\").date() \n",
        "            <= dt.datetime.strptime(date, '%Y_%m_%d').date()\n",
        "             <= dt.datetime.strptime(end_date, \"%Y-%m-%d\").date()):\n",
        "                files.append(os.path.join(folderpath, filename))\n",
        "# The final list is read, and each of the individual IDs is stored in a collective\n",
        "# set of IDs. Duplicates are removed.\n",
        "ids = set()\n",
        "for filename in files:\n",
        "    with open(filename) as f:\n",
        "        # The files are of the format: [id1,id2,id3,...,idn]\n",
        "        # Remove the brackets and split on commas\n",
        "        for i in f.readline().strip('][').replace(\" \", \"\").split(\",\"):\n",
        "            ids.add(i) \n",
        "# Number of tweets read.\n",
        "print(round((len(ids)/1000000), 3), \"million unique tweets.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BDEuBH7LPTn",
        "colab_type": "text"
      },
      "source": [
        "### Save configuration into a file\n",
        "All the IDs are read into a single set in the previous code block using the specified configuration. The ID Output file stores all the IDs in a single file so that the configuration blocks don't have to be run again. In case the program unexpectedly stops, you can just run the code for Initialization and then the code for Hydration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4EfPTrnLO0c",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter ID output file {run: \"auto\"}\n",
        "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
        "# The set of IDs is stored in this file.\n",
        "with open(final_tweet_ids_filename, \"w+\") as f:\n",
        "    for id in ids:\n",
        "        f.write('%s\\n' % id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxFa0jOTKbBw",
        "colab_type": "text"
      },
      "source": [
        "# Hydrate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBBH-a4WK1JM",
        "colab_type": "text"
      },
      "source": [
        "### Set up output file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U74uUnemI6h9",
        "colab_type": "text"
      },
      "source": [
        "The final_tweet_ids_filename should be exactly the same as the ID output file from the Configuration block. If this file does not exist in the working directory, you have to re-run the Configuration block.\n",
        "\n",
        "Please also keep the output_filename the same in case the code is halted. That way, tweets already hydrated aren't re-hydrated for no reason. \n",
        "\n",
        "Also, please do not remove the .txt file created after running the Hydrate block until all the data is converted to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ATxyEfSLBK1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Set up Directory { run: \"auto\"}\n",
        "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
        "output_filename = \"output.csv\" #@param {type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG9cS-aoW0Wy",
        "colab_type": "text"
      },
      "source": [
        "The time for this code will depend on how many tweets you want to “hydrate”. Also, be advise of the Tweet API limit, the code will “go to sleep” once the limit is reach and automatically continue. \n",
        "You can leave this code running in Google Colab for a max of 12hrs. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFwYd7m58WR3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "a0f5997b-c11c-4ba4-b43d-098b06730aaf"
      },
      "source": [
        "import jsonlines, json\n",
        "# Stores hydrated tweets here as jsonl objects\n",
        "# Contains one json object per line\n",
        "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
        "ids = []\n",
        "with open(final_tweet_ids_filename, \"r\") as ids_file:\n",
        "    ids = ids_file.read().split()\n",
        "hydrated_tweets = []\n",
        "ids_to_hydrate = set(ids)\n",
        "\n",
        "# Looks at the output file for already hydrated tweets\n",
        "if os.path.isfile(output_json_filename):\n",
        "    with jsonlines.open(output_json_filename, \"r\") as reader:\n",
        "        for i in reader.iter(type=dict, skip_invalid=True):\n",
        "            # These tweets have already been hydrated. So remove them from ids_to_hydrate\n",
        "            hydrated_tweets.append(i)\n",
        "            ids_to_hydrate.remove(i[\"id_str\"])\n",
        "print(\"Total IDs: \" + str(len(ids)) + \", IDs to hydrate: \" + str(len(ids_to_hydrate)))\n",
        "print(\"Hydrated: \" + str(len(hydrated_tweets)))\n",
        "\n",
        "count = len(hydrated_tweets)\n",
        "start_index = count # The index from where tweets haven't been saved to the output_json_file\n",
        "# Stores hydrated tweets to output_json_file every num_save iterations.\n",
        "num_save  = 1000\n",
        "\n",
        "# Now, use twarc and start hydrating\n",
        "for tweet in t.hydrate(ids_to_hydrate):\n",
        "    hydrated_tweets.append(tweet)\n",
        "    count += 1\n",
        "    # If num_save iterations have passed,\n",
        "    if (count % num_save) == 0:\n",
        "        # Open the output file\n",
        "        # NOTE: Even if the code stops during IO, only tweets from the current iteration are lost.\n",
        "        # Older tweets are preserved as the file is written in append mode.\n",
        "        with jsonlines.open(output_json_filename, \"a\") as writer:\n",
        "            print(\"Started IO\")\n",
        "            # Now write the tweets from start_index. The other tweets don't have to be written\n",
        "            # as they were already written in a previous iteration or run.\n",
        "            for hydrated_tweet in hydrated_tweets[start_index:]:\n",
        "                writer.write(hydrated_tweet)\n",
        "            print(\"Finished IO\")\n",
        "        print(\"Saved \" + str(count) + \" hydrated tweets.\")\n",
        "        # Now, since everything has been written. Reset start_index\n",
        "        start_index = count\n",
        "# There might be tweets unwritten in the last iteration if the count is not a multiple of num_tweets.\n",
        "# In that case, just write out the remainder of tweets.\n",
        "if count != start_index:\n",
        "    print(\"Here with start_index\", start_index)\n",
        "    with jsonlines.open(output_json_filename, \"a\") as writer:\n",
        "        for hydrated_tweet in hydrated_tweets[start_index:]:\n",
        "           writer.write(hydrated_tweet)   "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eace3a9d801d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Stores hydrated tweets here as jsonl objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Contains one json object per line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_json_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_filename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jsonlines'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6O-KWKnbcGt",
        "colab_type": "text"
      },
      "source": [
        "## Convert JSONL to CSV\n",
        "Data is stored in  output_json_file from the previous code block. This now converts the jsonl .txt file into a csv file. Note that the column names required is stored as a list in the code.\n",
        "\n",
        "Note that a few of the columns are actually json objects (for example, user or entities). You will have to clean these objects into 1D data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x68cvh5AJCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert jsonl to csv\n",
        "import csv, jsonlines\n",
        "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
        "# These are the column name that are selected to be stored in the csv\n",
        "keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
        "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
        "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
        "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
        "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
        "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
        "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
        "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
        "          \"quoted_status_permalink\"]\n",
        "hydrated_tweets = []\n",
        "# Reads the current tweets\n",
        "with jsonlines.open(output_json_filename, \"r\") as reader:\n",
        "    for i in reader.iter(type=dict, skip_invalid=True):\n",
        "        hydrated_tweets.append(i)\n",
        "# Writes them out\n",
        "with  open(output_filename, \"w+\") as output_file:\n",
        "    d = csv.DictWriter(output_file, keyset)\n",
        "    d.writeheader()\n",
        "    d.writerows(hydrated_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvrm8g7NHNs",
        "colab_type": "text"
      },
      "source": [
        "Your data is now stored in output_filename. If you want to re-run  this notebook, please copy this file to a different directory."
      ]
    }
  ]
}